{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0gqa7b2O9jkk",
      "metadata": {
        "id": "0gqa7b2O9jkk"
      },
      "source": [
        "# Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72cb5363-170d-4a71-b76d-7352dbc57eb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72cb5363-170d-4a71-b76d-7352dbc57eb0",
        "outputId": "c0f4aee1-e89c-4550-aa8a-6567ccac52f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FSoCy3rO4Sg5",
      "metadata": {
        "id": "FSoCy3rO4Sg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e32e63-4f74-46da-e4a4-410446ba70d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mamba-ssm\n",
            "  Downloading mamba_ssm-1.2.0.post1.tar.gz (34 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.2.1+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (24.0)\n",
            "Collecting ninja (from mamba-ssm)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting einops (from mamba-ssm)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->mamba-ssm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->mamba-ssm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->mamba-ssm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->mamba-ssm)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->mamba-ssm)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->mamba-ssm)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->mamba-ssm)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->mamba-ssm)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->mamba-ssm)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->mamba-ssm)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->mamba-ssm)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba-ssm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (1.25.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba-ssm) (1.3.0)\n",
            "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: mamba-ssm\n",
            "  Building wheel for mamba-ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba-ssm: filename=mamba_ssm-1.2.0.post1-cp310-cp310-linux_x86_64.whl size=137750683 sha256=b264292652a34fb9dd0ce880a34a4407ba7256a3338388d056769ec29a4581c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/6e/60/ddd5c574b5793a30028f2cabdacd2a3ec2276edaaa8c00fd35\n",
            "Successfully built mamba-ssm\n",
            "Installing collected packages: ninja, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, einops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, mamba-ssm\n",
            "Successfully installed einops-0.7.0 mamba-ssm-1.2.0.post1 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install mamba-ssm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd1064cb-f3d7-4e21-9ff8-c782fc6943a8",
      "metadata": {
        "id": "bd1064cb-f3d7-4e21-9ff8-c782fc6943a8"
      },
      "outputs": [],
      "source": [
        "!pip install causal-conv1d>=1.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "466097f0-b3ec-4bb2-86d7-d58ba71f245c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "466097f0-b3ec-4bb2-86d7-d58ba71f245c",
        "outputId": "b7c2c4e6-45d8-48ab-fda4-6777f0799532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.2-py3-none-any.whl.metadata (64 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.4.0)\n",
            "Downloading torch_geometric-2.5.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nYNcwfD7BgYJ",
      "metadata": {
        "id": "nYNcwfD7BgYJ"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CRDIvsFcBjZw",
      "metadata": {
        "id": "CRDIvsFcBjZw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "import networkx as nx\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time"
      ],
      "metadata": {
        "id": "zY6XNFm3KKnk"
      },
      "id": "zY6XNFm3KKnk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob"
      ],
      "metadata": {
        "id": "-nyG7fVnL2jo"
      },
      "id": "-nyG7fVnL2jo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "PlrZ0Q9OPpbi",
      "metadata": {
        "id": "PlrZ0Q9OPpbi"
      },
      "source": [
        "# em-user Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1AjbUktLSsFI",
      "metadata": {
        "id": "1AjbUktLSsFI"
      },
      "source": [
        "## Download Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0WyyIbMHPo08",
      "metadata": {
        "id": "0WyyIbMHPo08"
      },
      "outputs": [],
      "source": [
        "use_gdrive=False\n",
        "if use_gdrive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  data_dir = \"/content/drive/MyDrive/Subgraph_Mamba/data\"\n",
        "else:\n",
        "  data_dir = \"./data\"\n",
        "\n",
        "# Create a directory to store the downloaded files\n",
        "folder_name = data_dir+ \"/em_user_files\"\n",
        "os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "# Change to the directory\n",
        "os.chdir(folder_name)\n",
        "\n",
        "# List of direct download links for the files in the Dropbox folder\n",
        "dropbox_links = [\n",
        "    \"https://www.dropbox.com/sh/zv7gw2bqzqev9yn/AAC8G8ZaykeBUArvaq6wyt06a/em_user/degree_sequence.txt?dl=1\",\n",
        "    \"https://www.dropbox.com/sh/zv7gw2bqzqev9yn/AADyQ9oCLtpH9pmEEKgHyFcAa/em_user/edge_list.txt?dl=1\",\n",
        "    \"https://www.dropbox.com/sh/zv7gw2bqzqev9yn/AAB41SEX2hftRuW-cZtbPS1Pa/em_user/ego_graphs.txt?dl=1\",\n",
        "    # \"https://www.dropbox.com/sh/zv7gw2bqzqev9yn/AACTVyhujDdAyQV0nFIuCDm8a/em_user/shortest_path_matrix.npy?dl=1\",\n",
        "    \"https://www.dropbox.com/sh/zv7gw2bqzqev9yn/AAC1SMrLr2ifNFUzgSCMlgfja/em_user/subgraphs.pth?dl=1\",\n",
        "    \"https://www.dropbox.com/sh/zv7gw2bqzqev9yn/AAD0-ni027DwmHfkDr7A_DkMa/em_user/gin_embeddings.pth?dl=1\",\n",
        "    \"https://www.dropbox.com/sh/zv7gw2bqzqev9yn/AAC1pP6F9Ws_-TIz7WcrhScJa/em_user/graphsaint_gcn_embeddings.pth?dl=1\"\n",
        "]\n",
        "\n",
        "# Download each file\n",
        "for i, link in enumerate(dropbox_links, start=1):\n",
        "    file_name = link.split(\"/\")[-1].split(\"?\")[0]\n",
        "    print(f\"Downloading {file_name}...\")\n",
        "    !wget $link -O $file_name\n",
        "\n",
        "# Change back to the original directory\n",
        "os.chdir(\"..\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5CAClKKMVue",
      "metadata": {
        "id": "c5CAClKKMVue"
      },
      "outputs": [],
      "source": [
        "# neighbors of each node in a dict\n",
        "with open(\"em_user_files/ego_graphs.txt\", \"r\") as file:\n",
        "    file_contents = file.read()\n",
        "    ego_graphs = json.loads(file_contents)\n",
        "neighbor_dict = {int(key): value for key, value in ego_graphs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gJrB1AWpzQUU",
      "metadata": {
        "id": "gJrB1AWpzQUU"
      },
      "outputs": [],
      "source": [
        "# degrees of each node in a tensor\n",
        "with open(\"em_user_files/degree_sequence.txt\", \"r\") as file:\n",
        "  file_contents=file.read()\n",
        "  degree_sequence = json.loads(file_contents)\n",
        "degrees = torch.zeros(len(degree_sequence))\n",
        "for key, value in degree_sequence.items():\n",
        "  degrees[int(key)]=value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v-3MymYfQr7z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-3MymYfQr7z",
        "outputId": "ec1763fa-4cf7-424d-f7ff-6a631344451c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4573417])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Read the edge list from the text file\n",
        "with open('em_user_files/edge_list.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Parse the edge list and create the edge tensor\n",
        "edge_list = []\n",
        "for line in lines:\n",
        "    # Split the line and extract source and target node IDs\n",
        "    source, target = map(int, line.strip().split())  # Assuming nodes are integers\n",
        "    edge_list.append((source, target))\n",
        "\n",
        "# Convert the edge list to a PyTorch tensor\n",
        "edge_tensor = torch.tensor(edge_list).T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu68cWGXNnLn",
        "outputId": "2ad5ba21-1dff-45fa-ec9b-234c4b8be872"
      },
      "id": "gu68cWGXNnLn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    0,     0,     0,  ..., 57325, 57326, 57330],\n",
              "        [   23,    34,    78,  ..., 57329, 57332, 57331]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "edge_tensor.to('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LfyKJW7NpKf",
        "outputId": "22c031f0-affa-415f-d990-cd24642b560c"
      },
      "id": "7LfyKJW7NpKf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    0,     0,     0,  ..., 57325, 57326, 57330],\n",
              "        [   23,    34,    78,  ..., 57329, 57332, 57331]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brIH7XEUdtwg",
      "metadata": {
        "id": "brIH7XEUdtwg"
      },
      "outputs": [],
      "source": [
        "pretrained_node_embeds = torch.load(\"em_user_files/gin_embeddings.pth\", torch.device('cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9HnZrcnDeJnC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HnZrcnDeJnC",
        "outputId": "0139a16c-052c-4a31-98ab-72fee6ca8b87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([57333, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "pretrained_node_embeds.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yEK56njRNhmc",
      "metadata": {
        "id": "yEK56njRNhmc"
      },
      "outputs": [],
      "source": [
        "embeddings = torch.cat((pretrained_node_embeds, torch.zeros(1, pretrained_node_embeds.shape[1])), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "437AifNzQ4tC",
      "metadata": {
        "id": "437AifNzQ4tC"
      },
      "source": [
        "## Read subgraph labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PNiIlND0Q74s",
      "metadata": {
        "id": "PNiIlND0Q74s"
      },
      "outputs": [],
      "source": [
        "def read_subgraphs(sub_f, split = True):\n",
        "    '''\n",
        "    Read subgraphs from file\n",
        "\n",
        "    Args\n",
        "       - sub_f (str): filename where subgraphs are stored\n",
        "\n",
        "    Return for each train, val, test split:\n",
        "       - sub_G (list): list of nodes belonging to each subgraph\n",
        "       - sub_G_label (list): labels for each subgraph\n",
        "    '''\n",
        "\n",
        "    # Enumerate/track labels\n",
        "    label_idx = 0\n",
        "    labels = {}\n",
        "\n",
        "\n",
        "    # Train/Val/Test subgraphs\n",
        "    train_sub_G = []\n",
        "    val_sub_G = []\n",
        "    test_sub_G = []\n",
        "\n",
        "    # Train/Val/Test subgraph labels\n",
        "    train_sub_G_label = []\n",
        "    val_sub_G_label = []\n",
        "    test_sub_G_label = []\n",
        "\n",
        "    # Train/Val/Test masks\n",
        "    train_mask = []\n",
        "    val_mask = []\n",
        "    test_mask = []\n",
        "\n",
        "    multilabel = False\n",
        "\n",
        "    # Parse data\n",
        "    with open(sub_f) as fin:\n",
        "        subgraph_idx = 0\n",
        "        for line in fin:\n",
        "            nodes = [int(n) for n in line.split(\"\\t\")[0].split(\"-\") if n != \"\"]\n",
        "            if len(nodes) != 0:\n",
        "                if len(nodes) == 1: print(nodes)\n",
        "                l = line.split(\"\\t\")[1].split(\"-\")\n",
        "                if len(l) > 1: multilabel = True\n",
        "                for lab in l:\n",
        "                    if lab not in labels.keys():\n",
        "                        labels[lab] = label_idx\n",
        "                        label_idx += 1\n",
        "                if line.split(\"\\t\")[2].strip() == \"train\":\n",
        "                    train_sub_G.append(nodes)\n",
        "                    train_sub_G_label.append([labels[lab] for lab in l])\n",
        "                    train_mask.append(subgraph_idx)\n",
        "                elif line.split(\"\\t\")[2].strip() == \"val\":\n",
        "                    val_sub_G.append(nodes)\n",
        "                    val_sub_G_label.append([labels[lab] for lab in l])\n",
        "                    val_mask.append(subgraph_idx)\n",
        "                elif line.split(\"\\t\")[2].strip() == \"test\":\n",
        "                    test_sub_G.append(nodes)\n",
        "                    test_sub_G_label.append([labels[lab] for lab in l])\n",
        "                    test_mask.append(subgraph_idx)\n",
        "                subgraph_idx += 1\n",
        "    if not multilabel:\n",
        "        train_sub_G_label = torch.tensor(train_sub_G_label).long().squeeze()\n",
        "        val_sub_G_label = torch.tensor(val_sub_G_label).long().squeeze()\n",
        "        test_sub_G_label = torch.tensor(test_sub_G_label).long().squeeze()\n",
        "\n",
        "    if len(val_mask) < len(test_mask):\n",
        "        return train_sub_G, train_sub_G_label, test_sub_G, test_sub_G_label, val_sub_G, val_sub_G_label\n",
        "\n",
        "    return train_sub_G, train_sub_G_label, val_sub_G, val_sub_G_label, test_sub_G, test_sub_G_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wgR8gJXeC8yG",
      "metadata": {
        "id": "wgR8gJXeC8yG"
      },
      "outputs": [],
      "source": [
        "train_sub_G, train_sub_G_label, val_sub_G, val_sub_G_label, test_sub_G, test_sub_G_label = read_subgraphs(\"em_user_files/subgraphs.pth\", split = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zioau11kE6t9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zioau11kE6t9",
        "outputId": "7f64ef34-6ff4-4c25-b0fb-c8436fce1895"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "61"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "len(train_sub_G[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UjVetmKIUbBL",
      "metadata": {
        "id": "UjVetmKIUbBL"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gLjCyBXfUbBL",
      "metadata": {
        "id": "gLjCyBXfUbBL"
      },
      "outputs": [],
      "source": [
        "class SubgraphDataset(Dataset):\n",
        "  def __init__(self, subgraph_list, subgraph_labels, degrees, neighbor_dict, seqlength=10_000, hops=3):\n",
        "    '''\n",
        "    subgraph_list: list of lists of ints\n",
        "    subgraph_labels: list of ints\n",
        "    degrees: tensor where degrees[node_id] is degree of that node\n",
        "    neighbor_dict: dictionary mapping node_id to ids of neighbors\n",
        "    seqlength: (optional) maximum sequence length. Append -1 to the start of sequences\n",
        "    hops: (optional) number of hops away from subgraph to sample\n",
        "    '''\n",
        "    self.subgraph_list = subgraph_list\n",
        "    self.subgraph_labels = subgraph_labels\n",
        "    self.degrees = degrees\n",
        "    self.neighbor_dict = neighbor_dict\n",
        "    self.seqlength=seqlength\n",
        "    self.hops=hops\n",
        "  def __len__(self):\n",
        "    return len(self.subgraph_list)\n",
        "  def __getitem__(self, idx):\n",
        "    '''\n",
        "    need to rewrite this to use _get_sequence_ids\n",
        "\n",
        "    could also get last_embedding which is 0-1 label of inclusion in the subgraph\n",
        "      idea: just do 1 for the last len(subgraph_list[idx]) indices of the tensor\n",
        "      idea: inside the _get_sequence_ids we can also return an inclusion vector so <- implementing this\n",
        "    '''\n",
        "    y = torch.Tensor(self.subgraph_labels[idx])\n",
        "    sequence = self._get_sequence_ids(idx)\n",
        "    padding_length = self.seqlength - len(sequence)\n",
        "    padding_tensor = torch.full((padding_length,), -1)\n",
        "    sequence = torch.cat((padding_tensor, sequence))\n",
        "    inclusion = self._get_inclusion(idx, sequence)\n",
        "    return sequence, inclusion, len(self.subgraph_list[idx]), y\n",
        "  def _sort_by_degree(self, node_ids):\n",
        "    '''\n",
        "    node_ids - tensor of the node ids of nodes we want to sort by degree\n",
        "    largest to smallest bc then we can reverse the tensor at the very last step\n",
        "    '''\n",
        "    degrees = self.degrees[node_ids]\n",
        "    return node_ids[torch.argsort(degrees, descending=True)]\n",
        "\n",
        "  def _get_neighbor_ids(self, sequence):\n",
        "    '''\n",
        "    sequence - tensor. this is the previous sequence\n",
        "\n",
        "    Let v be a node k-1 hops away from the subgraph.\n",
        "    Then the neighbors of v are sorted by degree but still kept together in the sequence.\n",
        "    '''\n",
        "    neighbor_lists=[]\n",
        "    for s in sequence:\n",
        "      if s.item() in self.neighbor_dict.keys() and s.item() not in self.current_explored:\n",
        "        self.current_explored.add(s.item())\n",
        "        neighbors = torch.IntTensor(self.neighbor_dict[s.item()])\n",
        "        neighbor_lists.append(self._sort_by_degree(neighbors))\n",
        "        self.current_seq_len+=len(neighbors)\n",
        "      if self.current_seq_len >= self.seqlength:\n",
        "        break\n",
        "    return torch.cat(neighbor_lists)\n",
        "  def _get_inclusion(self, idx, sequence):\n",
        "    '''\n",
        "    idx: int, index for ith subgraph in the dataset\n",
        "    sequence: tensor, 1d, with node IDs\n",
        "    returns: tensor with 0 and 1, same shape as sequence.\n",
        "\n",
        "    the returned tensor has 1 if the node ID in sequence at that position is included in subgraph.\n",
        "    '''\n",
        "    subgraph = torch.IntTensor(self.subgraph_list[idx])\n",
        "    inclusion = (sequence.unsqueeze(1) == subgraph).any(dim=1).to(torch.float)\n",
        "    return inclusion\n",
        "  def _get_sequence_ids(self, idx):\n",
        "    '''\n",
        "    idx: int, index ith subgraph in dataset\n",
        "    returns 1-d tensor of length at most self.seqlength\n",
        "\n",
        "    Generate a sequence of node IDs associated with a subgraph.\n",
        "    Nodes further away from the subgraph appear earlier in the sequence.\n",
        "    Nodes with the same distance from the subgraph are grouped by path to subgraph.\n",
        "    Nodes within same group are sorted by degree.\n",
        "    '''\n",
        "    sequences = [self._sort_by_degree(torch.IntTensor(self.subgraph_list[idx]))]\n",
        "    self.current_seq_len = len(sequences[0])\n",
        "    self.current_explored=set()\n",
        "    for i in range(self.hops):\n",
        "      if self.current_seq_len < self.seqlength:\n",
        "        neighbor_ids = self._get_neighbor_ids(sequences[i])\n",
        "        sequences.append(neighbor_ids[~torch.isin(neighbor_ids, sequences[i])])\n",
        "        self.current_seq_len=sum(len(sequences[j]) for j in range(len(sequences)))\n",
        "    return torch.cat(sequences).flip(dims=[0])[-self.seqlength:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_k4h64_993a_",
      "metadata": {
        "id": "_k4h64_993a_"
      },
      "source": [
        "# Model Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lgjjLiuZJ2a4",
      "metadata": {
        "id": "lgjjLiuZJ2a4"
      },
      "outputs": [],
      "source": [
        "from mamba_ssm import Mamba\n",
        "device = torch.device('cuda:0')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model with GCNConv"
      ],
      "metadata": {
        "id": "mDwWCGLhTf_X"
      },
      "id": "mDwWCGLhTf_X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ewp4e_61CWek",
      "metadata": {
        "id": "Ewp4e_61CWek"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.utils import degree, sort_edge_index, to_dense_batch\n",
        "from mamba_ssm import Mamba\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "\n",
        "class SubgraphMamba(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, mamba_dim, num_classes, embeddings, edge_tensor, num_mamba_layers=1):\n",
        "        super(SubgraphMamba, self).__init__()\n",
        "        self.edge_tensor = edge_tensor\n",
        "        self.embeddings = embeddings\n",
        "        self.mamba = Mamba(\n",
        "            d_model=mamba_dim,\n",
        "            d_state=16,\n",
        "            d_conv=4,\n",
        "            expand=2,\n",
        "        )\n",
        "        self.mamba_layers=[]\n",
        "        for m in range(num_mamba_layers):\n",
        "          mamba=Mamba(d_model=hidden_dim, d_state=16, d_conv=4, expand=2)\n",
        "          mamba.to(device)\n",
        "          self.mamba_layers.append(mamba)\n",
        "        # uncomment below when you have a good GPU\n",
        "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        # self.fc1 = nn.Linear(num_features, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, sequence, inclusion, subgraph_size):\n",
        "        # Preprocessing\n",
        "        emb = embeddings[sequence]\n",
        "        last_entry=inclusion.unsqueeze(-1)\n",
        "        emb = torch.cat((emb, last_entry), dim=-1)\n",
        "\n",
        "        # start with linear layer projection\n",
        "        emb = self.conv1(emb, self.edge_tensor, None)\n",
        "        emb = self.fc1(emb)\n",
        "\n",
        "        # Mamba layers\n",
        "        for mamba in self.mamba_layers:\n",
        "          emb=mamba(emb)\n",
        "\n",
        "        # a bit awkward but I don't know how else to compute my aggregation\n",
        "        agg = []\n",
        "        for e, size in zip(emb, subgraph_size):\n",
        "          agg.append(torch.sum(e[-size:], axis=0)/size)\n",
        "        emb = torch.stack(agg)\n",
        "\n",
        "        # Linear layer for final class, assume 0-1 classification\n",
        "        emb = self.fc2(emb)\n",
        "        emb = F.sigmoid(emb)\n",
        "\n",
        "        return emb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## No GCNConv"
      ],
      "metadata": {
        "id": "SN4Mb6_QTivZ"
      },
      "id": "SN4Mb6_QTivZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.utils import degree, sort_edge_index, to_dense_batch\n",
        "from mamba_ssm import Mamba\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "\n",
        "class SubgraphMamba(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, mamba_dim, num_classes, embeddings, num_mamba_layers=1):\n",
        "        super(SubgraphMamba, self).__init__()\n",
        "        # self.edge_tensor = edge_tensor\n",
        "        self.embeddings = embeddings\n",
        "        self.mamba = Mamba(\n",
        "            d_model=mamba_dim,\n",
        "            d_state=16,\n",
        "            d_conv=4,\n",
        "            expand=2,\n",
        "        )\n",
        "        self.mamba_layers=[]\n",
        "        for m in range(num_mamba_layers):\n",
        "          mamba=Mamba(d_model=hidden_dim, d_state=16, d_conv=4, expand=2)\n",
        "          mamba.to(device)\n",
        "          self.mamba_layers.append(mamba)\n",
        "        # uncomment below when you have a good GPU\n",
        "        # self.conv1 = GCNConv(num_features, hidden_dim)\n",
        "        # self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.fc1 = nn.Linear(num_features, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, sequence, inclusion, subgraph_size):\n",
        "        # Preprocessing\n",
        "        emb = embeddings[sequence]\n",
        "        last_entry=inclusion.unsqueeze(-1)\n",
        "        emb = torch.cat((emb, last_entry), dim=-1)\n",
        "\n",
        "        # start with linear layer projection\n",
        "        # emb = self.conv1(emb, self.edge_tensor, None)\n",
        "        emb = self.fc1(emb)\n",
        "\n",
        "        # Mamba layers\n",
        "        for mamba in self.mamba_layers:\n",
        "          emb=mamba(emb)\n",
        "\n",
        "        # a bit awkward but I don't know how else to compute my aggregation\n",
        "        agg = []\n",
        "        for e, size in zip(emb, subgraph_size):\n",
        "          agg.append(torch.sum(e[-size:], axis=0)/size)\n",
        "        emb = torch.stack(agg)\n",
        "\n",
        "        # Linear layer for final class, assume 0-1 classification\n",
        "        emb = self.fc2(emb)\n",
        "        emb = F.sigmoid(emb)\n",
        "\n",
        "        return emb"
      ],
      "metadata": {
        "id": "1wUVrxwTTkpT"
      },
      "id": "1wUVrxwTTkpT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking Model"
      ],
      "metadata": {
        "id": "Wtm6H5T5Tlau"
      },
      "id": "Wtm6H5T5Tlau"
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embeddings.to(device)\n",
        "edge_tensor = edge_tensor.to(device)\n",
        "train_dataset = SubgraphDataset(train_sub_G, train_sub_G_label, degrees, neighbor_dict)\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "val_dataset = SubgraphDataset(val_sub_G, val_sub_G_label, degrees, neighbor_dict)\n",
        "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=True)"
      ],
      "metadata": {
        "id": "92Aqea7uPPs1"
      },
      "id": "92Aqea7uPPs1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence, inclusion, subgraph_size, _ = next(iter(train_loader))\n",
        "model = SubgraphMamba(129, 5, 5, 2, embeddings, num_mamba_layers=2)\n",
        "\n",
        "embeddings = embeddings.to(device)\n",
        "sequence = sequence.to(device)\n",
        "inclusion = inclusion.to(device)\n",
        "subgraph_size = subgraph_size.to(device)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "JWAXtJB2eXjU"
      },
      "id": "JWAXtJB2eXjU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(sequence, inclusion, subgraph_size)"
      ],
      "metadata": {
        "id": "X6yG5lYTfAiy"
      },
      "id": "X6yG5lYTfAiy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "v690yYdN-UN-",
      "metadata": {
        "id": "v690yYdN-UN-"
      },
      "source": [
        "# Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cfcf1c3-a3ea-42fa-903f-c655733276a2",
      "metadata": {
        "id": "8cfcf1c3-a3ea-42fa-903f-c655733276a2"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    max_acc = 0.00\n",
        "    patience_cnt = 0\n",
        "    val_acc_values = []\n",
        "    best_epoch = 0\n",
        "\n",
        "    t = time()\n",
        "    model.train()\n",
        "    for epoch in range(5):\n",
        "        loss_train = 0.0\n",
        "        correct = 0\n",
        "        for i, data in enumerate(train_loader):\n",
        "            sequence, inclusion, subgraph_size, y = data\n",
        "            optimizer.zero_grad()\n",
        "            sequence = sequence.to(device)\n",
        "            inclusion = inclusion.to(device)\n",
        "            subgraph_size = subgraph_size.to(device)\n",
        "            y=y.to(device)\n",
        "            out = model(sequence, inclusion, subgraph_size)\n",
        "\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_train += loss.item()\n",
        "            pred = out.max(dim=1)[1]\n",
        "            correct += pred.eq(y).sum().item()\n",
        "        acc_train = correct / len(train_loader.dataset)\n",
        "\n",
        "        acc_val, loss_val = compute_test(val_loader)\n",
        "        print('Epoch: {:04d}'.format(epoch + 1), 'loss_train: {:.4f}'.format(loss_train),\n",
        "              'acc_train: {:.4f}'.format(acc_train),\n",
        "              'acc_val: {:.4f}'.format(acc_val))\n",
        "\n",
        "        val_acc_values.append(acc_val)\n",
        "        torch.save(model.state_dict(), '{}.pth'.format(epoch))\n",
        "        if val_acc_values[-1] > max_acc:\n",
        "            max_acc = val_acc_values[-1]\n",
        "            best_epoch = epoch\n",
        "            patience_cnt = 0\n",
        "        else:\n",
        "            patience_cnt += 1\n",
        "\n",
        "        if patience_cnt == 100:\n",
        "            break\n",
        "\n",
        "        files = glob.glob('*.pth')\n",
        "        for f in files:\n",
        "            epoch_nb = int(f.split('.')[0])\n",
        "            if epoch_nb < best_epoch:\n",
        "                os.remove(f)\n",
        "\n",
        "    files = glob.glob('*.pth')\n",
        "    for f in files:\n",
        "        epoch_nb = int(f.split('.')[0])\n",
        "        if epoch_nb > best_epoch:\n",
        "            os.remove(f)\n",
        "    print('Optimization Finished! Total time elapsed: {:.6f}'.format(time() - t))\n",
        "\n",
        "    return best_epoch\n",
        "\n",
        "\n",
        "def compute_test(loader):\n",
        "    model.eval()\n",
        "    correct = 0.0\n",
        "    loss_test = 0.0\n",
        "    for data in loader:\n",
        "        sequence, inclusion, subgraph_size, y = data\n",
        "        sequence = sequence.to(device)\n",
        "        inclusion = inclusion.to(device)\n",
        "        subgraph_size = subgraph_size.to(device)\n",
        "        y=y.to(device)\n",
        "        out = model(sequence, inclusion, subgraph_size)\n",
        "        pred = out.max(dim=1)[1]\n",
        "        correct += pred.eq(y).sum().item()\n",
        "        loss_test += criterion(out, y).item()\n",
        "    return correct / len(loader.dataset), loss_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embeddings.to(device)\n",
        "# edge_tensor = edge_tensor.to(device)\n",
        "train_dataset = SubgraphDataset(train_sub_G, train_sub_G_label, degrees, neighbor_dict)\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "val_dataset = SubgraphDataset(val_sub_G, val_sub_G_label, degrees, neighbor_dict)\n",
        "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=True)"
      ],
      "metadata": {
        "id": "7ORmKVJVMxRc"
      },
      "id": "7ORmKVJVMxRc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84c3a85d-553e-402e-8c26-34f172fa7226",
      "metadata": {
        "id": "84c3a85d-553e-402e-8c26-34f172fa7226",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91ae3f88-f15e-4ce8-be5f-2f7cb8d7aafd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SubgraphMamba(\n",
              "  (mamba): Mamba(\n",
              "    (in_proj): Linear(in_features=16, out_features=64, bias=False)\n",
              "    (conv1d): Conv1d(32, 32, kernel_size=(4,), stride=(1,), padding=(3,), groups=32)\n",
              "    (act): SiLU()\n",
              "    (x_proj): Linear(in_features=32, out_features=33, bias=False)\n",
              "    (dt_proj): Linear(in_features=1, out_features=32, bias=True)\n",
              "    (out_proj): Linear(in_features=32, out_features=16, bias=False)\n",
              "  )\n",
              "  (fc1): Linear(in_features=129, out_features=16, bias=True)\n",
              "  (fc2): Linear(in_features=16, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "model = SubgraphMamba(129, 32, 32, 2, embeddings, num_mamba_layers=2)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay = 0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model=train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dzNZgTXMsQS",
        "outputId": "48a41474-8acd-4edb-98b3-2965320a7e9e"
      },
      "id": "7dzNZgTXMsQS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 15.9594 acc_train: 0.4912 acc_val: 0.4490\n",
            "Epoch: 0002 loss_train: 15.9507 acc_train: 0.4912 acc_val: 0.4490\n",
            "Epoch: 0003 loss_train: 15.9534 acc_train: 0.4912 acc_val: 0.4490\n",
            "Epoch: 0004 loss_train: 15.9594 acc_train: 0.4912 acc_val: 0.4490\n",
            "Epoch: 0005 loss_train: 15.9557 acc_train: 0.4912 acc_val: 0.4490\n",
            "Optimization Finished! Total time elapsed: 20.979321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "966e1b96-a9e2-439d-ad15-09ed96f85d49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "966e1b96-a9e2-439d-ad15-09ed96f85d49",
        "outputId": "38289e10-895d-40e8-a61d-57ee25da0389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0001 loss_train: 19.1891 acc_train: 0.6011 acc_val: 0.6216\n",
            "Epoch: 0002 loss_train: 18.3392 acc_train: 0.6169 acc_val: 0.6486\n",
            "Epoch: 0003 loss_train: 17.5171 acc_train: 0.6360 acc_val: 0.6577\n",
            "Epoch: 0004 loss_train: 17.0323 acc_train: 0.6652 acc_val: 0.6847\n",
            "Epoch: 0005 loss_train: 17.0640 acc_train: 0.6708 acc_val: 0.6937\n",
            "Epoch: 0006 loss_train: 16.8557 acc_train: 0.6910 acc_val: 0.6847\n",
            "Epoch: 0007 loss_train: 16.6482 acc_train: 0.6955 acc_val: 0.7117\n",
            "Epoch: 0008 loss_train: 16.3526 acc_train: 0.7146 acc_val: 0.7117\n",
            "Epoch: 0009 loss_train: 16.3752 acc_train: 0.7124 acc_val: 0.7207\n",
            "Epoch: 0010 loss_train: 16.3334 acc_train: 0.7146 acc_val: 0.7207\n",
            "Epoch: 0011 loss_train: 15.9987 acc_train: 0.7292 acc_val: 0.7658\n",
            "Epoch: 0012 loss_train: 15.8299 acc_train: 0.7258 acc_val: 0.7928\n",
            "Epoch: 0013 loss_train: 15.7283 acc_train: 0.7326 acc_val: 0.7838\n",
            "Epoch: 0014 loss_train: 15.7212 acc_train: 0.7427 acc_val: 0.7207\n",
            "Epoch: 0015 loss_train: 15.8624 acc_train: 0.7303 acc_val: 0.7568\n",
            "Epoch: 0016 loss_train: 15.4124 acc_train: 0.7472 acc_val: 0.7928\n",
            "Epoch: 0017 loss_train: 15.4245 acc_train: 0.7562 acc_val: 0.7928\n",
            "Epoch: 0018 loss_train: 16.1429 acc_train: 0.7382 acc_val: 0.7568\n",
            "Epoch: 0019 loss_train: 15.5484 acc_train: 0.7348 acc_val: 0.8018\n",
            "Epoch: 0020 loss_train: 15.6843 acc_train: 0.7438 acc_val: 0.7477\n",
            "Epoch: 0021 loss_train: 15.5756 acc_train: 0.7483 acc_val: 0.7928\n",
            "Epoch: 0022 loss_train: 15.5479 acc_train: 0.7449 acc_val: 0.7838\n",
            "Epoch: 0023 loss_train: 15.3604 acc_train: 0.7472 acc_val: 0.8108\n",
            "Epoch: 0024 loss_train: 15.2354 acc_train: 0.7472 acc_val: 0.7658\n",
            "Epoch: 0025 loss_train: 15.2970 acc_train: 0.7483 acc_val: 0.8018\n",
            "Epoch: 0026 loss_train: 15.2564 acc_train: 0.7562 acc_val: 0.7928\n",
            "Epoch: 0027 loss_train: 15.0292 acc_train: 0.7629 acc_val: 0.7748\n",
            "Epoch: 0028 loss_train: 15.9313 acc_train: 0.7270 acc_val: 0.8108\n",
            "Epoch: 0029 loss_train: 15.2204 acc_train: 0.7506 acc_val: 0.7838\n",
            "Epoch: 0030 loss_train: 15.4296 acc_train: 0.7539 acc_val: 0.7928\n",
            "Optimization Finished! Total time elapsed: 10.580350\n",
            "Test set results, loss = 2.118571, accuracy = 0.723214\n"
          ]
        }
      ],
      "source": [
        "best_model = train()\n",
        "    # Restore best model for test set\n",
        "model.load_state_dict(torch.load('{}.pth'.format(best_model)))\n",
        "test_acc, test_loss = compute_test(test_loader)\n",
        "print('Test set results, loss = {:.6f}, accuracy = {:.6f}'.format(test_loss, test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fa6834a-ea1d-45dd-b7d0-b0a536e3b935",
      "metadata": {
        "id": "8fa6834a-ea1d-45dd-b7d0-b0a536e3b935"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "nYNcwfD7BgYJ",
        "1AjbUktLSsFI",
        "437AifNzQ4tC",
        "UjVetmKIUbBL"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}