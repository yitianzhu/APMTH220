/home/gridsan/yzhu/AM220/SubgraphMamba/dataset.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(self.subgraph_labels[idx], dtype=torch.long)
Current date and time: 2024-04-11 14:59:17.838328
{'dataset_name': 'ppi_bp', 'learning_rate': 0.0005, 'weight_decay': 0.01, 'batch_size': 32, 'epochs': 50, 'patience': 100, 'hidden_dim': 16, 'mamba_dim': 16, 'seqlength': 3000, 'hops': 2, 'num_classes': 6, 'multilabel': False, 'embeddings_filename': 'graphsaint_gcn_embeddings.pth'}
[9339]
{'metabolism': 0, 'development': 1, 'signal_transduction': 2, 'stress_death': 3, 'cell_org': 4, 'transport': 5}
Epoch: 0001 loss_train: 70.9165 acc_train: 0.2217 acc_val: 0.2562
Epoch: 0002 loss_train: 69.2090 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0003 loss_train: 68.0736 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0004 loss_train: 67.8096 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0005 loss_train: 67.7853 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0006 loss_train: 67.7011 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0007 loss_train: 67.7466 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0008 loss_train: 67.7534 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0009 loss_train: 67.6660 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0010 loss_train: 67.6797 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0011 loss_train: 67.6693 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0012 loss_train: 67.6458 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0013 loss_train: 67.6197 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0014 loss_train: 67.6149 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0015 loss_train: 67.5979 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0016 loss_train: 67.6442 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0017 loss_train: 67.5881 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0018 loss_train: 67.5606 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0019 loss_train: 67.5426 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0020 loss_train: 67.5291 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0021 loss_train: 67.4878 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0022 loss_train: 67.4319 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0023 loss_train: 67.4816 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0024 loss_train: 67.4306 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0025 loss_train: 67.3844 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0026 loss_train: 67.3632 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0027 loss_train: 67.3320 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0028 loss_train: 67.3813 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0029 loss_train: 67.3332 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0030 loss_train: 67.2942 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0031 loss_train: 67.2900 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0032 loss_train: 67.2491 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0033 loss_train: 67.2020 acc_train: 0.3058 acc_val: 0.2562
Epoch: 0034 loss_train: 67.1830 acc_train: 0.3058 acc_val: 0.2562
Epoch: 0035 loss_train: 67.1675 acc_train: 0.3035 acc_val: 0.2562
Epoch: 0036 loss_train: 67.1664 acc_train: 0.3042 acc_val: 0.2562
Epoch: 0037 loss_train: 67.0882 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0038 loss_train: 67.0627 acc_train: 0.3042 acc_val: 0.2562
Epoch: 0039 loss_train: 67.0477 acc_train: 0.3058 acc_val: 0.2562
Epoch: 0040 loss_train: 66.9854 acc_train: 0.3082 acc_val: 0.2562
Epoch: 0041 loss_train: 66.9236 acc_train: 0.3090 acc_val: 0.2562
Epoch: 0042 loss_train: 66.9507 acc_train: 0.3050 acc_val: 0.2625
Epoch: 0043 loss_train: 66.9408 acc_train: 0.3027 acc_val: 0.2625
Epoch: 0044 loss_train: 66.8920 acc_train: 0.3011 acc_val: 0.2625
Epoch: 0045 loss_train: 66.8686 acc_train: 0.3003 acc_val: 0.2562
Epoch: 0046 loss_train: 66.8101 acc_train: 0.3019 acc_val: 0.2625
Epoch: 0047 loss_train: 66.8441 acc_train: 0.3090 acc_val: 0.2625
Epoch: 0048 loss_train: 66.8034 acc_train: 0.2995 acc_val: 0.2625
Epoch: 0049 loss_train: 66.7410 acc_train: 0.3082 acc_val: 0.2625
Epoch: 0050 loss_train: 66.7731 acc_train: 0.2995 acc_val: 0.2562
Optimization Finished! Total time elapsed: 342.787425
