/home/gridsan/yzhu/AM220/SubgraphMamba/dataset.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y = torch.tensor(self.subgraph_labels[idx], dtype=torch.long)
Current date and time: 2024-04-11 14:59:17.838333
{'dataset_name': 'ppi_bp', 'learning_rate': 0.0005, 'weight_decay': 0.01, 'batch_size': 32, 'epochs': 50, 'patience': 100, 'hidden_dim': 16, 'mamba_dim': 16, 'seqlength': 3000, 'hops': 2, 'num_classes': 6, 'multilabel': False, 'embeddings_filename': 'graphsaint_gcn_embeddings.pth'}
[9339]
{'metabolism': 0, 'development': 1, 'signal_transduction': 2, 'stress_death': 3, 'cell_org': 4, 'transport': 5}
Epoch: 0001 loss_train: 69.7667 acc_train: 0.2508 acc_val: 0.2562
Epoch: 0002 loss_train: 68.5293 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0003 loss_train: 67.9222 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0004 loss_train: 67.7589 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0005 loss_train: 67.7475 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0006 loss_train: 67.7289 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0007 loss_train: 67.7103 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0008 loss_train: 67.6684 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0009 loss_train: 67.7452 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0010 loss_train: 67.7177 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0011 loss_train: 67.6843 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0012 loss_train: 67.6967 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0013 loss_train: 67.6536 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0014 loss_train: 67.6841 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0015 loss_train: 67.6740 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0016 loss_train: 67.6409 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0017 loss_train: 67.6513 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0018 loss_train: 67.5768 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0019 loss_train: 67.6279 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0020 loss_train: 67.6078 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0021 loss_train: 67.6092 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0022 loss_train: 67.6097 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0023 loss_train: 67.5493 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0024 loss_train: 67.5336 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0025 loss_train: 67.5371 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0026 loss_train: 67.4316 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0027 loss_train: 67.4760 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0028 loss_train: 67.3924 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0029 loss_train: 67.3936 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0030 loss_train: 67.3731 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0031 loss_train: 67.2840 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0032 loss_train: 67.2254 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0033 loss_train: 67.1772 acc_train: 0.3035 acc_val: 0.2562
Epoch: 0034 loss_train: 67.1561 acc_train: 0.3035 acc_val: 0.2562
Epoch: 0035 loss_train: 67.1038 acc_train: 0.3058 acc_val: 0.2562
Epoch: 0036 loss_train: 66.8710 acc_train: 0.3090 acc_val: 0.2562
Epoch: 0037 loss_train: 67.1314 acc_train: 0.3050 acc_val: 0.2562
Epoch: 0038 loss_train: 67.0028 acc_train: 0.3066 acc_val: 0.2562
Epoch: 0039 loss_train: 66.9913 acc_train: 0.3027 acc_val: 0.2562
Epoch: 0040 loss_train: 66.7727 acc_train: 0.3011 acc_val: 0.2562
Epoch: 0041 loss_train: 66.9504 acc_train: 0.2972 acc_val: 0.2562
Epoch: 0042 loss_train: 67.0691 acc_train: 0.3027 acc_val: 0.2562
Epoch: 0043 loss_train: 66.8820 acc_train: 0.3058 acc_val: 0.2562
Epoch: 0044 loss_train: 66.7972 acc_train: 0.3082 acc_val: 0.2625
Epoch: 0045 loss_train: 66.7799 acc_train: 0.3074 acc_val: 0.2625
Epoch: 0046 loss_train: 66.7215 acc_train: 0.3042 acc_val: 0.2562
Epoch: 0047 loss_train: 66.7849 acc_train: 0.2987 acc_val: 0.2562
Epoch: 0048 loss_train: 66.7749 acc_train: 0.3019 acc_val: 0.2562
Epoch: 0049 loss_train: 66.6755 acc_train: 0.3035 acc_val: 0.2562
Epoch: 0050 loss_train: 66.6888 acc_train: 0.3035 acc_val: 0.2562
Optimization Finished! Total time elapsed: 343.674949
